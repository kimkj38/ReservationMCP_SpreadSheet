import asyncio
import nest_asyncio
import json
import os
import platform
import uuid
import time
from typing import List, Optional, Dict, Any, AsyncGenerator
from datetime import datetime

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import uvicorn

from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_core.messages.ai import AIMessageChunk
from langchain_core.messages.tool import ToolMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "/home/ubuntu/Desktop/mcp_sheets.json"
load_dotenv(override=True)

# Pydantic ëª¨ë¸ ì •ì˜ (OpenAI API í˜¸í™˜)
class ChatMessage(BaseModel):
    role: str = Field(..., description="ë©”ì‹œì§€ ì—­í•  (system, user, assistant)")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©")

class ChatCompletionRequest(BaseModel):
    model: str = Field(default="gpt-4o-mini", description="ì‚¬ìš©í•  ëª¨ë¸")
    messages: List[ChatMessage] = Field(..., description="ëŒ€í™” ë©”ì‹œì§€ë“¤")
    stream: bool = Field(default=False, description="ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€")
    temperature: float = Field(default=0.1, ge=0, le=2, description="ì‘ë‹µ ì°½ì˜ì„±")
    max_tokens: Optional[int] = Field(default=16000, description="ìµœëŒ€ í† í° ìˆ˜")

class ChatCompletionChoice(BaseModel):
    index: int
    message: Optional[ChatMessage] = None
    delta: Optional[Dict[str, Any]] = None
    finish_reason: Optional[str] = None

class ChatCompletionResponse(BaseModel):
    id: str
    object: str
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Optional[Dict[str, int]] = None

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """<ROLE>
You are a hospital reservation agent with an ability to use tools. 
You will be given a question and you will use the tools to answer the question.
Pick the most relevant tool to answer the question. 
If you are failed to answer the question, try different tools to get context.
Your answer should be very polite and professional.
</ROLE>

----

<INSTRUCTIONS>
Step 1: Analyze the question
- Analyze user's question and final goal.
- If the user's question is consist of multiple sub-questions, split them into smaller sub-questions.

Step 2: Pick the most relevant tool
- Pick the most relevant tool to answer the question.
- If you are failed to answer the question, try different tools to get context.

Step 3: Answer the question
- Answer the question in the same language as the question.
- Your answer should be very polite and professional.

Step 4: Provide the source of the answer(if applicable)
- If you've used the tool, provide the source of the answer.
- Valid sources are either a website(URL) or a document(PDF, etc).

Guidelines:
- If you've used the tool, your answer should be based on the tool's output(tool's output is more important than your own knowledge).
- If you've used the tool, and the source is valid URL, provide the source(URL) of the answer.
- Skip providing the source if the source is not URL.
- Answer in the same language as the question.
- Answer should be concise and to the point.
- Avoid response your output with any other information than the answer and the source.  
</INSTRUCTIONS>

<PROCESS>
A. ì˜ˆì•½í•˜ê¸°
    1. ë¬¸ì„œIDëŠ” "1lXs3JrOuvBSew2EJUZhEeaEQfGaSqIcuKcVicOkRxMQ" ì‹œíŠ¸ì˜ ì´ë¦„ì€ "ì‹œíŠ¸1"ì…ë‹ˆë‹¤.
    2. ì„±ëª…, ìƒë…„ì›”ì¼, ì˜ˆì•½ì¼, ì˜ˆì•½ì‹œê°„ì€ í•„ìˆ˜ìš”ì†Œì…ë‹ˆë‹¤. ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´ ì •ì¤‘í•˜ê²Œ ìš”ì²­í•˜ì„¸ìš”.
    3. í•„ìš”í•œ ì •ë³´ê°€ ë‹¤ ìˆ˜ì§‘ë˜ì—ˆë‹¤ë©´ get_sheet_data íˆ´ì„ í™œìš©í•˜ì—¬ ê¸°ì¡´ ì˜ˆì•½ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”.
    4. ë™ì¼í•œ ì˜ˆì•½ì¼ê³¼ ì˜ˆì•½ì‹œê°„ì— ì´ë¯¸ ì˜ˆì•½ëœ ì •ë³´ê°€ ì¡´ì¬í•œë‹¤ë©´, ì˜ˆì•½ì„ ì ˆëŒ€ ì§„í–‰í•˜ì§€ ë§ˆì„¸ìš”.
       - ì¤‘ë³µ ì˜ˆì•½ì´ ê°ì§€ë˜ì—ˆì„ ê²½ìš° ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•˜ì„¸ìš”:
         - "í•´ë‹¹ ì‹œê°„ì—ëŠ” ì´ë¯¸ ì˜ˆì•½ì´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì‹œê°„ëŒ€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
       - ê°€ëŠ¥í•œ ë‹¤ë¥¸ ì‹œê°„ëŒ€ë¥¼ 2~3ê°œ ì¶”ì²œí•˜ì„¸ìš”.
    5. ì¤‘ë³µ ì˜ˆì•½ì´ ì—†ìŒì„ í™•ì¸í•œ í›„, ë¹ˆ í–‰ì„ íƒìƒ‰í•˜ì—¬ í•´ë‹¹ ìœ„ì¹˜ì— ì •ë³´ë¥¼ ê¸°ì…í•˜ì„¸ìš”.
    6. update_cells íˆ´ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ì˜ˆì•½ì •ë³´ë¥¼ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”.
B. ì˜ˆì•½ ì·¨ì†Œí•˜ê¸°
    1. ë¬¸ì„œIDëŠ” "1lXs3JrOuvBSew2EJUZhEeaEQfGaSqIcuKcVicOkRxMQ" ì‹œíŠ¸ì˜ ì´ë¦„ì€ "ì‹œíŠ¸1"ì…ë‹ˆë‹¤.
    2. get_sheet_data íˆ´ì„ í™œìš©í•˜ì—¬ ì·¨ì†Œë¥¼ ìš”ì²­ë°›ì€ ì´ë¦„ì„ íƒìƒ‰í•©ë‹ˆë‹¤.
    3. í•´ë‹¹ ì´ë¦„ì´ ìˆëŠ” í–‰ì˜ ì •ë³´ë¥¼ ì§€ì›ë‹ˆë‹¤.
    4. í•˜ë‚˜ì˜ í–‰ì´ ë¹„ê²Œ ë˜ë¯€ë¡œ ê·¸ ì•„ë˜ ë‚´ìš©ë“¤ì„ ìœ„ë¡œ í•œ ì¹¸ì”© ë‹¹ê¹ë‹ˆë‹¤.
    
</PROCESS>

----

<OUTPUT_FORMAT>
(concise answer to the question)

**Source**(if applicable)
- (source1: valid URL)
- (source2: valid URL)
- ...
</OUTPUT_FORMAT>
"""

class HospitalReservationAgent:
    def __init__(self):
        self.agent = None
        self.client = None
        self.sessions = {}  # ì„¸ì…˜ë³„ ì„¤ì • ì €ì¥
        
    async def initialize(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        try:
            print("ğŸ”„ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # config.json íŒŒì¼ ë¡œë“œ
            with open("config.json", 'r', encoding='utf-8') as f:
                mcp_config = json.load(f)
                print(f"âœ… config.json ë¡œë“œ ì™„ë£Œ")
            
            # MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.client = MultiServerMCPClient(mcp_config)
            await self.client.__aenter__()
            tools = self.client.get_tools()
            print(f"ğŸ”§ ë„êµ¬ ë¡œë“œ ì™„ë£Œ: {len(tools)}ê°œ")
            
            # OpenAI ëª¨ë¸ ì‚¬ìš©
            model = ChatOpenAI(
                model='gpt-4o-mini',
                temperature=0.1,
                max_tokens=16000,
            )
            print("ğŸ¤– OpenAI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì—ì´ì „íŠ¸ ìƒì„±
            self.agent = create_react_agent(
                model,
                tools,
                checkpointer=MemorySaver(),
                prompt=SYSTEM_PROMPT,
            )
            print("ğŸ¯ ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.client:
            try:
                await self.client.__aexit__(None, None, None)
                print("ğŸ§¹ MCP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_session_config(self, session_id: str) -> RunnableConfig:
        """ì„¸ì…˜ë³„ ì„¤ì • ë°˜í™˜"""
        if session_id not in self.sessions:
            self.sessions[session_id] = RunnableConfig(
                configurable={"thread_id": session_id}
            )
        return self.sessions[session_id]
    
    async def stream_chat(self, messages: List[ChatMessage], session_id: str) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ëŒ€í™”í•˜ê¸°"""
        if not self.agent:
            raise Exception("ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_message = None
            for msg in reversed(messages):
                if msg.role == "user":
                    user_message = msg.content
                    break
            
            if not user_message:
                yield "ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return
            
            human_message = HumanMessage(content=user_message)
            config = self.get_session_config(session_id)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            async for chunk in self.agent.astream(
                {"messages": [human_message]}, 
                stream_mode="messages",
                config=config
            ):
                # ì—ì´ì „íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬
                if 'tool_calls' in chunk[0].additional_kwargs:
                    print()
                else:
                    token = chunk[0].content
                    yield token
                
        except Exception as e:
            error_msg = f"ëŒ€í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            yield error_msg

# ì „ì—­ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
agent_instance = None

async def get_agent():
    """ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global agent_instance
    if agent_instance is None:
        agent_instance = HospitalReservationAgent()
        await agent_instance.initialize()
    return agent_instance

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
app = FastAPI(
    title="ë³‘ì› ì˜ˆì•½ ì—ì´ì „íŠ¸ API",
    description="Google Sheetsë¥¼ í™œìš©í•œ ë³‘ì› ì˜ˆì•½ ê´€ë¦¬ ì‹œìŠ¤í…œ",
    version="1.0.0"
)

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
    print("ğŸš€ ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # ìš´ì˜ ì²´ì œë³„ ì´ë²¤íŠ¸ ë£¨í”„ ì •ì±… ì„¤ì •
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    # nest_asyncio ì ìš©
    try:
        nest_asyncio.apply()
        print("ğŸ”„ nest_asyncio ì ìš© ì™„ë£Œ")
    except:
        print("â„¹ï¸ nest_asyncio ì ìš© ê±´ë„ˆëœ€")
    
    # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    await get_agent()
    print("âœ… ì„œë²„ ì‹œì‘ ì™„ë£Œ!")

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    global agent_instance
    if agent_instance:
        await agent_instance.cleanup()
    print("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "ë³‘ì› ì˜ˆì•½ ì—ì´ì „íŠ¸ API",
        "version": "1.0.0",
        "endpoints": {
            "chat": "/v1/chat/completions",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI í˜¸í™˜ ì±„íŒ… ì™„ë£Œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        agent = await get_agent()
        
        # ì„¸ì…˜ ID ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìš”ì²­ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ì‚¬ìš©ì ì¸ì¦ì„ í†µí•´ ì„¤ì •)
        session_id = str(uuid.uuid4())
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        if request.stream:
            async def generate_stream():
                response_id = f"chatcmpl-{uuid.uuid4().hex}"
                created = int(time.time())
                
                # ìŠ¤íŠ¸ë¦¼ ì‹œì‘
                yield f"data: {json.dumps({'id': response_id, 'object': 'chat.completion.chunk', 'created': created, 'model': request.model, 'choices': [{'index': 0, 'delta': {'role': 'assistant', 'content': ''}, 'finish_reason': None}]})}\n\n"
                
                # ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                async for content_chunk in agent.stream_chat(request.messages, session_id):
                    if content_chunk.strip():  # ë¹ˆ ë‚´ìš© ì œì™¸
                        chunk_data = {
                            'id': response_id,
                            'object': 'chat.completion.chunk',
                            'created': created,
                            'model': request.model,
                            'choices': [{
                                'index': 0,
                                'delta': {'content': content_chunk},
                                'finish_reason': None
                            }]
                        }
                        yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                
                # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                end_chunk = {
                    'id': response_id,
                    'object': 'chat.completion.chunk',
                    'created': created,
                    'model': request.model,
                    'choices': [{
                        'index': 0,
                        'delta': {},
                        'finish_reason': 'stop'
                    }]
                }
                yield f"data: {json.dumps(end_chunk)}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/plain; charset=utf-8"
                }
            )
        
        # ì¼ë°˜ ì‘ë‹µ
        else:
            full_response = ""
            async for content_chunk in agent.stream_chat(request.messages, session_id):
                full_response += content_chunk
            
            response = ChatCompletionResponse(
                id=f"chatcmpl-{uuid.uuid4().hex}",
                object="chat.completion",
                created=int(time.time()),
                model=request.model,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=ChatMessage(role="assistant", content=full_response),
                        finish_reason="stop"
                    )
                ],
                usage={
                    "prompt_tokens": len(str(request.messages)),
                    "completion_tokens": len(full_response),
                    "total_tokens": len(str(request.messages)) + len(full_response)
                }
            )
            
            return response
            
    except Exception as e:
        print(f"âŒ API ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(e)}")

# ê¸°ë³¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš©)
@app.post("/chat")
async def simple_chat(message: dict):
    """ê°„ë‹¨í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        agent = await get_agent()
        session_id = str(uuid.uuid4())
        
        user_message = message.get("message", "")
        if not user_message:
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        messages = [ChatMessage(role="user", content=user_message)]
        
        response = ""
        async for chunk in agent.stream_chat(messages, session_id):
            response += chunk
        
        return {"response": response}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    print("ğŸ¥ ë³‘ì› ì˜ˆì•½ ì—ì´ì „íŠ¸ FastAPI ì„œë²„")
    print("=" * 50)
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "api_google_sheet:app",  # ì‹¤ì œ íŒŒì¼ëª…ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
        host="0.0.0.0",
        port=8090,
        reload=True,
        log_level="info"
    )

